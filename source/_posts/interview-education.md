[TOC]



## 自我介绍(待补充，关于各家公司的技术难点在哪里)

- 我是一名Java架构师，8年工作经验，擅长复杂业务领域的分布式系统架构，有3年多的团队管理经验

- 我在13年本科毕业，目前为止供职过3家互联网公司，第一家是上海聚力有限公司（PPTV），第二家是阿里巴巴，第三家是旦悦科技有限公司（在职）

- 我在PPTV任职过1年半主要从事的是移动端的后端核心开发，负责安卓及iOS App端的接口标准化解决方案

- 我在阿里任职过3年半，主要负责阿里toB端付款及财务域的开发。

- 我在旦悦任职已经2年，目前依然在职，旦悦是一家互联网在线教育公司，我负责的是旦悦所有后端业务系统的整体架构相关的事宜，同时兼任教务领域相关的开发。



## 教务云

教务云项目的目标是支撑企业级教育机构能够迅速线上化的项目，这块项目目前主要分了 三层架构

第一层是基础设计层，主要用于支撑客户端的基础建设，包括音视频服务，白板涂鸦，课件播放，录屏回放等功能

第二层是业务共享域，主要是抽离教务的底层业务，提供通用的服务能力，分为了业务配置，课程管理，运营平台，教师管理

第三层是业务应用域，主用是用于基于业务共享域的服务编排达到实现不同教务业务场景，比如像排班选课，自由选课，



### 项目难点

1. 如何保证业务的灵活性和扩展性的同时又兼顾项目的可维护性。
   1. 目前线上教育行业，针对不同的学科，衍生了多种多样的排课方式，有1对1的辅导，有1对多的大班课，有学生自由选课的方式，也有进班排课的方式，这是一个复杂点，需要对整个排课体系有一个合理的底层抽象；
   2. 另一个是教师的资源利用方式，首先企业会同时存在多种排课方式支撑学生不同诉求的场景，那不同的排课方式，教师资源利用上会存在冲突；同时一个教师可能存在支持多个不同课程体系的场景，我们需要有效的利用教师资源同时，降低多业务并发的场景下逻辑的耦合，提升可维护性

<font color="red"> 1.1 参考COLOA架构，对新系统做架构分层，将排课方案划分到业务应用域，将老师时间的管理，课程体系的管理划分到业务共享域，保证排课方案的可扩展性，能快速迭代新的排课方案</font>

<font color="red"> 1.2 在业务共享域定义业务扩展点，在业务应用实现业务扩展域的服务提供方，利用Nacos解耦业务共享域对业务应用域的依赖</font>

​		业务端配置租户信息，基于租户信息通过服务发现的形式寻找租户服务

​		namespace+group+service锁定唯一的业务扩展点调用方

​		基于元数据模块获取可用接口，来判断业务应用域是否实现了扩展点

<font color="red"> 1.3 在业务共享域抽离出业务规则模块，实现不同业务应用域定义不同的业务执行规则，简化业务应用域的对接方案</font>

<font color="red"> 1.4 定义业务事件，提供业务消息总线，提供运营平台，助力业务同学管理学生老师，并保证事务的最终一致性</font>

<font color="red"> 1.5 基于教师的课程包权限去管控教师的授课权限，而不依赖于业务扩展域限制，做到教师资源在底层共享；在教师域和课程域定义资源预留协议和资源使用协议，解耦业务扩展域的互相依赖</font>





2. 性能优化方案
   1. 数据堆积瓶颈：课程记录模块每月产生15W节课, 每年产生200W以上的课，因此课程记录，以及日志数据堆积会影响到数据库的性能
   2. 高并发场景：学生自由选课的场景下每天晚上18点会存在一个抢课的高并发场景，请求峰值达到300-400每秒

<font color="red">2.1 最多保留2年的课程记录，每个月清洗一次历史课程记录到backup数据库，查询历史数据库要依赖数据平台；建立MySQL主从数据库，做部分业务场景的读写分离。</font>

<font color="red">2.2 基于Redis HINCRBY 方案更新教师资源库存，实现Redis的主从复制，基于Redis的请求队列异步回写到数据库</font>

<font color="red">2.3 业务请求基于CAS实现乐观锁方案，部分请求采用异步化执行，基于SAGA模式保证事务的一致性</font>

<font color="red">2.4 减少数据库请求频率, 基本不可变数据的使用缓存做冗余，如教师的基本信息，学生的基本信息</font>

<font color="red">2.5 使用Sentinel做流控，实现流量控制、熔断降级方案</font>



### 选择COLA架构的原因

调研过Apache Isis，Jdon Framework等DDD架构，但普遍存在几种问题

1. 架构实现方案太重，非轻量级框, 需要依赖一些用不上的东西
2. 文档太少，迭代时间久远，学习成本高

另外我本人在阿里的时候用过COLA架构的前身，所以使用上会更加得心应手



### 什么是DDD架构

领域驱动设计，是一种设计理念，旨在构建满足复杂业务场景下可持续发展的业务系统。

领域驱动设计在第一步就剥离了数据模型和数据行为，只考虑业务模型

传统架构下存在的问题：

- 业务模型间依赖性与业务领域边界定义不清晰，并且随着需求迭代升级，系统的业务抽象模型容易畸形扭曲，迭代效率下降
- 业务领域专家与开发之间存在代沟，难以在业务本质和代码实现上达成共识

如何构建可持续发展的业务项目

1. 产出通用、共享的团队模型语言，基于模型语言交流，交流越频繁，越有利于大家对模型的认知达成一致
2. 搭建领域模型，这有利于开发不会基于功能去编写代码
3. 使用分层架构使领域模型具有内聚性，且只依赖于下层

使用合适的设计模式设计应用架构

1. 防腐层避免被腐蚀
2. 使用上下文实现域与域之间的交流
3. 使用扩展点解耦依赖



## 分布式架构

生态化分布式服务体系架构，基于SpringCloudAlibaba

- 基础组件
  - SpringCloudAlibaba
    - **Nacos**(ConfigCenter+服务发现+管理，PHP+JAVA+Python)
    - **[Sentinel](https://github.com/alibaba/Sentinel)**（流量控制，熔断降级，负载均衡）
    - **[RocketMQ](https://rocketmq.apache.org/)**（分布式消息系统）
    - Seata
      - AT（没用）
      - TCC模式(没用)
      - Saga模式
      - 消息模式
      - XA事务(Seata)
      - 本地消息表
  - XXL-JOB
  - 全链路追踪(Arms, 底层基于JAVA Agent)
  - Canal
  - MyCat(未来会用，多租户解决方案)
- 基础服务
  - 消息服务中心
    - 钉钉
    - 短信
    - App推送
  - CRM中心
  - 用户中心(Redis)
    - 用户统一登录体系
    - 用户权限管理体系
  - 大数据平台(Spark)
  - 发布系统(Jenkins)



## 消息中间件的原理

分布式问题：

分布式事务，分布式锁，远程调用

### Spring Cloud Nacos

Spring Cloud Nacos用于实现微服务的配置中心+服务注册与发现

----

配置中心数据模型由namespace+DataId+group组成

1. Spring Cloud基于bootstrap.yaml配置从云端拉取应用配置，且该配置会依次拉取不含profiles的配置和含profiles的配置
2. namespace用于区分多租户的场景或者线上测试环境隔离
3. `dataId=${spring.application.name}-${spring.profiles.active}.${nacos.file-extension}`
4. Group用于区分应用组别

----

服务发现与服务注册

1. 数据模型在配置中心的模型的基础上增加了Metadata(元数据)的概念，可以用于定于接口版本，流量管控策略，路由规则等

2. 负载均衡策略在客户端实现NacosRule

   1. 同集群优先调用策略

   2. 基于权重分配调用频率

      

### Sentinel

![Sentinel-features-overview](https://user-images.githubusercontent.com/9434884/50505538-2c484880-0aaf-11e9-9ffc-cbaaef20be2b.png)

健康监控：

​	health page监控，接口性能监控，流量监控

流控：

​	基于接口做QPS规则限制，如每秒限制QPS为50个

熔断：

​	响应时间超出2秒以上，则触发熔断机制，每过5秒允许通过一个请求，若该请求耗时正常则结束熔断，否则再次触发熔断。

​	自定义熔断返回业务异常





工作原理：https://github.com/alibaba/Sentinel/wiki/Sentinel%E5%B7%A5%E4%BD%9C%E4%B8%BB%E6%B5%81%E7%A8%8B

***Context***

调用链路上下文，基于ThreadLoacl实现，记录调用链路的基本信息

***Entry***

在Sentinel中，一个资源对应了Entry，一次资源调用对应了一个Entry实例

创建Entry的方式有三种，一种是基于注解，一种是基于API，一种跟组件依赖的适配器，例如web端的URL

***Slot***

每一个 Entry 创建的时候，同时也会创建一系列功能插槽（slot chain），所有的slot都包含在`ProcessorSlotChain`里面

`NodeSelectorSlot` 负责收集资源的路径，并将这些资源的调用路径，以树状结构存储起来，用于根据调用路径来限流降级；
`ClusterBuilderSlot` 则用于存储资源的统计信息以及调用者信息，例如该资源的 RT, QPS, thread count 等等，这些信息将用作为多维度限流，降级的依据；
`StatisticSlot` 则用于记录、统计不同纬度的 runtime 指标监控信息；
`FlowSlot` 则用于根据预设的限流规则以及前面 slot 统计的状态，来进行流量控制；
`AuthoritySlot` 则根据配置的黑白名单和调用来源信息，来做黑白名单控制；
`DegradeSlot` 则通过统计信息以及预设的规则，来做熔断降级；
`SystemSlot` 则通过系统的状态，例如 load1 等，来控制总的入口流量；

### Dubbo

----



## 分布式事务的解决方案

## TCC模式

TCC 模式，不依赖于底层数据资源的事务支持：

- 一阶段 try 行为：调用 **自定义** 的 try 逻辑。
- 二阶段 commit 行为：调用 **自定义** 的 commit 逻辑。
- 二阶段 rollback 行为：调用 **自定义** 的 rollback 逻辑。

所谓 TCC 模式，是指支持把 **自定义** 的分支事务纳入到全局事务的管理中

## SAGA模式

Saga模式是SEATA提供的长事务解决方案，在Saga模式中，业务流程中每个参与者都提交本地事务，当出现某一个参与者失败则补偿前面已经成功的参与者，一阶段正向服务和二阶段补偿服务都由业务开发实现

## XA模式

XA 规范 是 X/Open 组织定义的分布式事务处理（DTP，Distributed Transaction Processing）标准

XA 规范 使用两阶段提交（2PC，Two-Phase Commit）来保证所有资源同时提交或回滚任何特定的事务

## 本地消息表

- 对业务请求实现本地异步化，标记业务状态为执行中

- 通过重试解决网络问题，接口支持幂等

- 通过业务回滚方法解决消息消费失败的场景

- 本地消息表告警机制

  ![img](https://static001.geekbang.org/infoq/a9/a9a98d328ba7df424c27ad5e437401c6.webp?x-oss-process=image/resize,p_80/format,jpg)

## RocketMQ事务消息

在本地事务执行前，发送事务消息prepare，本地事务执行成功，发送事务消息commit，实现分布式事务最终一致性。如果事务消息commit失败，RocketMQ会回查消息发送者确保消息正常提交，如果步骤5执行失败，进行重试，达到最终一致性

![img](https://static001.geekbang.org/infoq/23/238286d2c31392bc8d4ca41a72071ec6.webp?x-oss-process=image/resize,p_80/format,jpg)



## 如何通过技术赋能实现大数据

- 数据分层：数据运营层（ODS）、数据仓库层/数据集市（DW）、数据产品层（APP）
- ETL封装
- 监控学习效果
- 监控学生的发声时长

## 遇见过哪些有挑战的复杂技术问题

## 跟业界其他的教务系统比较起来如何



## 你的成长在哪里



## 业务中台战略思考



## 分布式基础理论

## 2PC

2PC(tow phase commit)两阶段提交。

所谓的两个阶段是指：第一阶段：准备阶段(投票阶段)和第二阶段：提交阶段（执行阶段）。

我们将提议的节点称为协调者(coordinator)，其他参与决议节点称为参与者(participants, 或cohorts)。

## 3PC

三阶段提交（Three-phase commit），是二阶段提交（2PC）的改进版本。

与两阶段提交不同的是，三阶段提交有两个改动点。

- 引入超时机制。同时在协调者和参与者中都引入超时机制。
- 在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。

 也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有`CanCommit`、`PreCommit`、`DoCommit`三个阶段。

## CAP

[一致性](https://baike.baidu.com/item/一致性/9840083)（Consistency）、[可用性](https://baike.baidu.com/item/可用性/109628)（Availability）、[分区容错性](https://baike.baidu.com/item/分区容错性/23734073)（Partition tolerance）。CAP 原则指的是，这三个[要素](https://baike.baidu.com/item/要素/5261200)最多只能同时实现两点，不可能三者兼顾。

![img](images/cap.png)

**CA without P：**如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但放弃P的同时也就意味着放弃了系统的扩展性，也就是分布式节点受限，没办法部署子节点，这是违背分布式系统设计的初衷的。传统的关系型数据库RDBMS：Oracle、MySQL就是CA。

**CP without A：**如果不要求A（可用），相当于每个请求都需要在服务器之间保持强一致，而P（分区）会导致同步时间无限延长(也就是等待数据同步完才能正常访问服务)，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。设计成CP的系统其实不少，最典型的就是分布式数据库，如Redis、HBase等。对于这些分布式数据库来说，数据的一致性是最基本的要求，因为如果连这个标准都达不到，那么直接采用关系型数据库就好，没必要再浪费资源来部署分布式数据库。

 **AP wihtout C：**要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。典型的应用就如某米的抢购手机场景，可能前几秒你浏览商品的时候页面提示是有库存的，当你选择完商品准备下单的时候，系统提示你下单失败，商品已售完。这其实就是先在 A（可用性）方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，虽然多少会影响一些用户体验，但也不至于造成用户购物流程的严重阻塞。

## BASE

基本可用（Basically Available）软状态（Soft State）最终一致性（Eventually Consistent）

即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）

## Paxos

选举制算法，保证分布式系统数据的强一致性

## 业务的目标价值，如何保障稳定性，亮点在哪里？

## 阿里巴巴中台战略思想与架构实战

1. 重复功能建设和维护带来的重复投资
2. 打通“烟囱”系统间交互的集成和协作成本高
3. 不利于业务的可持续发展和沉淀
4. 松耦合服务保证可复用
5. 基于服务编排助力业务快速响应和创新

## 你认为你的优缺点在哪里

我是一个对业务敏感度非常高的人，所以作为业务开发，我对业务的抽象能力和前瞻性等方面，都比大部分开发要强很多；

并且我非常专业技术体系的发展，我经常看阿里云开发者社区的文章，同时会偶尔关注海外云的发展和相关的文章

我个人的管理能力会相对薄弱，没有大型团队的管理经验，更多的是3-5人小团队的管理经验。因为基本我待过的所有团队都是敏捷开发+快速迭代的模式

## 你为什么要跳槽

话术1：主要是基于职业规划的考虑，目前在公司需要同时兼顾技术体系的升级和业务体系拓展，并不符合我的职业规划。那我个人希望能够找到一个更能发挥我能力优势的地方，去深入挖掘业务体系，改变行业的趋势；技术体系升级对我而言只是为了满足业务体系需要而去做的，我希望能够有更专业的团队去负责技术体系的架构。

话术2：主要是目前公司的整体工作氛围相对偏慢，加班比较少，我觉得自己还年轻，所以希望能找到更有激情的团队，去深耕一个业务领域，创造价值；

## 团队管理心得分享

